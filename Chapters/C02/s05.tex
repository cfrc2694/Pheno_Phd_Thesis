\section{ML enhanced signal-background discrimination}
\label{sec:machine_learning}

As shown in Sec.~\ref{sec:power_analysis}, the sensitivity of a search depends on optimally separating signal and background processes~\cite{Cowan:2011,Guest2018}. Traditional ``cut-based'' analyses, which apply sequential selection criteria to individual observables, cannot fully exploit the discriminatory information contained in the high-dimensional feature space of collision events~\cite{Hastie2009,Guest2018}. In this approach, for an event described by a feature vector $\mathbf{x} = (x_1, x_2, \ldots, x_N)$, selection criteria are applied in the form~\cite{Hoecker2007}
\begin{equation}
\text{Selection:} \quad x_1 > c_1 \,\text{AND}\, x_2 > c_2 \,\text{AND}\, \ldots \,\text{AND}\, x_N > c_N,
\end{equation}
where $c_i$ represents an specific  constrain (cut) imposed on the dataset. 

This method has several limitations that become apparent when considering correlations among kinematic variables~\cite{Hastie2009,Guest2018}. A typical LHC event contains a large number of observables, and an optimal discriminator must consider these variables and their correlations simultaneously~\cite{Hoecker2007,pedregosa_scikit-learn_2011}. Tight event selection criteria might severely limit the available phase space, discarding events that are signal-like in multivariate space but fall just outside univariate boundaries~\cite{Hastie2009,Guest2018}. The challenge of dimensionality also arises, as optimizing many cuts becomes unstable and is prone to statistical fluctuations~\cite{Hastie2009,friedman_greedy_2001}. Finally, sequential cuts cannot capture non-linear relationships and complex decision boundaries that often provide the strongest discrimination~\cite{Hastie2009,Guest2018}.

Supervised learning addresses these limitations directly~\cite{Hastie2009,Guest2018}. It learns a function $f(\mathbf{x})$ that maps the high-dimensional input space to a continuous score, approximating the posterior probability~\cite{Cranmer2015,Platt1999}:
\begin{equation}
f: \mathbb{R}^N \rightarrow [0,1], \quad f(\mathbf{x}) \approx P(\text{signal} \mid \mathbf{x}).
\end{equation}
It means that the task is to assign a label:
\begin{equation}
f(\mathbf{x}) =
\begin{cases}
1 & \text{if the event originates from the signal process}, \\
0 & \text{if the event originates from the background}.
\end{cases}
\end{equation}
This score incorporates correlations and nonlinearities present in the training data, providing a powerful and continuous discriminant~\cite{Hastie2009,Guest2018,Cranmer2015,pedregosa_scikit-learn_2011}.

Logistic regression, SVMs~\cite{Cortes1995}, Random Forests~\cite{Breiman2001}, and boosted trees~\cite{friedman_greedy_2001,chen_xgboost_2016} are common choices; see \cite{Hastie2009,Hoecker2007} for comparisons and practical guidance. 

In particular, logistic regression is often used as a baseline due to its simplicity and transparency. It assumes a linear decision boundary in the feature space, with the discriminant
\begin{equation}
f(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b),
\end{equation}
where $\mathbf{w}$ are model parameters and $b$ is a bias term. The model learns a linear combination $z = \mathbf{w} \cdot \mathbf{x} + b$ that best separates the two classes. Then, it applies a sigmoid function to map this unbounded score to a probability-like output between  $[0,1]$ 

\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}}.
\end{equation}

As illustrated in Fig.~\ref{fig:logistic_explanation}, a direct linear fit to the binary labels (orange line) is unbounded and can produce values outside the physically meaningful range $[0,1]$. The sigmoid transformation (purple curve) constrains the output while preserving the ordering of events and providing faster convergence to the asymptotic values: as $z \to -\infty$, $\sigma(z) \to 0$ (background region), and as $z \to +\infty$, $\sigma(z) \to 1$ (signal region).

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/logistic_regression_explanation.pdf}
    \caption{Illustration of logistic regression on a toy dataset with two classes drawn from Gaussian distributions. The orange line shows an ordinary linear regression fit $f_{\text{lin}}(\mathbf{x}) = \mathbf{w}_{\text{lin}} \cdot \mathbf{x} + b_{\text{lin}}$ to the binary labels, which is unbounded and can produce values outside $[0,1]$. The purple curve shows the logistic regression output $f(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$, where the sigmoid function constrains the output to $[0,1]$, while maintaining a smooth, monotonic response. The dashed Gaussian curves illustrate the underlying class distributions. A decision threshold (horizontal dashed line) at $f(\mathbf{x}) = 0.55$ would classify events above it as signal and below as background, though in practice the full continuous distribution is used for statistical inference.}
    \label{fig:logistic_explanation}
\end{figure} 

While a hard decision threshold (e.g., classifying events with $f(\mathbf{x}) > 0.55$ as signal) can be applied, modern analyses exploit the full continuous output distribution rather than collapsing it to a binary decision. Figure~\ref{fig:logistic_distribution} shows the resulting distributions of $f(\mathbf{x})$ for signal and background events after applying the trained logistic regression model. Signal events concentrate near $f(\mathbf{x}) \approx 1$, while background events peak near $f(\mathbf{x}) \approx 0$, with some overlap in intermediate regions reflecting the inherent classification uncertainty. 

This approach—using the complete output distribution rather than a single cut—is central to the analysis strategy employed throughout this work. As detailed in Sec.~\ref{ssec:ml_workflow}, the classifier output is binned into a histogram, and each bin contributes to the final sensitivity calculation through the likelihood framework of Sec.~\ref{sec:power_analysis}. This maximizes the statistical power by preserving all discriminating information: events with intermediate scores, which would be ambiguous under a hard threshold, still contribute meaningfully to the analysis through their likelihood contributions.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/logistic_regression_distribution.pdf}
    \caption{Distribution of the logistic regression output $f(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$ for signal (green) and background (blue) events in the toy dataset. Rather than applying a single decision threshold, the analysis uses the full shape of these distributions as input to the statistical inference procedure described in Sec.~\ref{sec:power_analysis}, maximizing the sensitivity by exploiting all available discriminating information across the entire output range.}
    \label{fig:logistic_distribution}
\end{figure}

While logistic regression cannot capture complex non-linear relationships, it remains useful when signal and background are approximately linearly separable, and it provides a clear reference point to compare against more sophisticated methods.

Tree-based ensembles are among the most widely used methods. To understand how they work, we first consider the building block: a \textit{decision tree}. This is a hierarchical model that recursively partitions the feature space through binary splits~\cite{Breiman2001,Hastie2009}. Starting from the root node containing all training events, the algorithm selects the feature and threshold that best separates signal from background according to a splitting criterion. This process continues recursively, where  each iteration is further split until a stopping condition is met (maximum depth, minimum events per node, or no further improvement). The terminal nodes, or \textit{leaves}, assign weights based on the number of nodes classified correctly as signal or background. For example, a tree might first be split on events passing a $p_T^{\text{leading jet}} > 150~\text{GeV}$ threshold. Subsequently, a $m_{jj} > 800~\text{GeV}$ criterion is imposed on these events. This creates a sequence of increasingly refined regions in feature space. While conceptually transparent, individual trees are highly sensitive to training data fluctuations in the dataset. Small changes might produce very different tree structures, making them a high-variance and low-bias learners~\cite{Hastie2009}. 

To address the high variance conundrum, we can combine many trees, trained with different subsets of data. These ensemble methods have been proven to be effective to average out individual errors. The Random Forest algorithm approaches this through \textit{bootstrap sampling}~\cite{Breiman2001}, a resampling technique that creates multiple training datasets from the original data. 

Given a dataset of $n$ events, each bootstrap sample is constructed by randomly drawing $n$ events \textit{with replacement}, meaning the same event can appear multiple times in a single sample. On average, each bootstrap sample contains approximately $63$\% of the unique original events, with the remaining $37$\% (called \textit{out-of-bag} or OOB events) left out. By training a separate decision tree on each bootstrap sample and additionally introducing random feature selection at each split, Random Forests combine these diverse predictors through simple averaging:
\begin{equation}
F(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} T_b(\mathbf{x}),
\end{equation}
where $B$ is the number of trees and $T_b(\mathbf{x})$ is the prediction of the $b$-th tree. For classification, this is typically the fraction of signal events in that leaf: if a leaf contains $80$ signal and $20$ background training events, then $T_b(\mathbf{x}) = 0.8$ for any test event reaching that leaf. The final Random Forest prediction $F(\mathbf{x})$ is simply the arithmetic mean of these $B$ individual tree predictions, effectively polling the entire ensemble to produce a robust, averaged probability estimate.
This approach reduces variance and mitigates overfitting through averaging. Random Forests are robust, parallelizable, and provide natural feature importance measures, though they typically require more trees than boosted methods and may yield slightly lower performance on well-tuned tasks.

Boosted Decision Trees (BDTs) have become a standard tool in particle physics because they balance interpretability and performance. Unlike Random Forests, which build trees independently in parallel, BDTs employ a fundamentally different strategy: it constructs an ensemble \textit{sequentially}, with each new tree specifically trained to correct the mistakes of the previous ensemble. This sequential, adaptive learning process is the defining characteristic of boosting methods.

The term \textit{weak learner} refers to a simple model that performs only slightly better than random guessing—for binary classification, this means achieving accuracy modestly above $50$\%. In the context of BDTs, weak learners are typically shallow decision trees (depth $3$–$6$), sometimes called \textit{stumps} when limited to just a few splits. While such a tree alone provides minimal discriminating power, the boosting algorithm iteratively combines many of these weak learners into a \textit{strong learner}—an ensemble that achieves high accuracy. This is the essence of boosting: transforming simple, individually weak classifiers into a powerful collective predictor through sequential refinement.

The boosting procedure works as follows. Initially, all training events are assigned equal weights. The first weak learner $h_1(\mathbf{x})$ is trained on this dataset and makes predictions. Events that are misclassified—signal events predicted as background or vice versa—are then assigned higher weights, while correctly classified events receive lower weights. The second tree $h_2(\mathbf{x})$ is trained on this reweighted dataset, forcing it to focus on the previously difficult cases. This process repeats: after each iteration $m$, events that remain misclassified have their weights increased, and the next tree $h_{m+1}(\mathbf{x})$ is added to specifically target these problematic regions of feature space. The final ensemble combines all trees through a weighted sum:
\begin{equation}
F_M(\mathbf{x}) = \sum_{m=1}^{M} \gamma_m h_m(\mathbf{x}),
\end{equation}
where $M$ is the total number of trees, $h_m(\mathbf{x})$ is the prediction of the $m$-th weak learner, and $\gamma_m$ is its weight in the ensemble. The weight $\gamma_m$ reflects the performance of tree $m$: more accurate trees receive larger weights and thus contribute more strongly to the final prediction. Each tree's prediction $h_m(\mathbf{x})$ is obtained similarly to Random Forests—by passing the event through the tree's binary splits until reaching a leaf—but here the tree has been trained specifically to correct errors from $F_{m-1}(\mathbf{x})$, the ensemble of all previous trees.

This \textit{sequential correction} or \textit{residual learning} is what distinguishes boosting from bagging methods like Random Forests. Rather than averaging independent predictions, boosting builds a progression of specialists: each new tree learns from the failures of its predecessors, gradually refining the decision boundary in regions where the current ensemble performs poorly. The result is a powerful classifier that adaptively allocates modeling capacity to the most challenging parts of the feature space. BDTs are robust to outliers and non-Gaussian distributions, handle mixed variable types naturally, and perform implicit feature selection, often revealing which observables are most discriminating. These properties explain their widespread adoption in LHC analyses.

Finally, Deep Neural Networks (DNNs) represent the most expressive class of models, capable of learning highly complex, hierarchical representations of the input data. A deep network with multiple hidden layers can be written as
\begin{equation}
f(\mathbf{x}) = \sigma^{(L)}\left(W^{(L)} \sigma^{(L-1)}\left( \cdots \sigma^{(1)}(W^{(1)} \mathbf{x} + b^{(1)}) \cdots \right) + b^{(L)}\right),
\end{equation}
where $L$ is the number of layers. DNNs excel at capturing intricate patterns and correlations, but they require large training datasets, careful regularization, and extensive hyperparameter tuning. Their black-box nature also complicates interpretability and the assessment of systematic uncertainties.

In practice, Random Forests are often used as robust, low-maintenance alternatives, while SVMs have largely been superseded by tree-based methods and neural networks due to scalability issues. DNNs can outperform other methods on very complex problems with sufficient data, but they demand significantly more computational resources. For most LHC searches, BDTs—particularly implementations such as XGBoost—provide the best balance between performance, interpretability, and computational efficiency. In Sec.~\ref{sec:ML}, and in Tab.~\ref{tab:ml_results}, we compare the performance of different methods and show that the gain in accuracy from DNNs is marginal compared to BDTs, while the training time is substantially larger.


\subsection{XGBoost: Optimized Gradient Boosting}
\label{ssec:xgboost}

\textit{XGBoost} (eXtreme Gradient Boosting) is a widely used implementation of gradient boosting that has become standard in high energy physics due to its efficiency, predictive power, and robustness. 

A single decision tree is a weak classifier: it partitions the feature space through binary splits but is highly sensitive to fluctuations in the training data. Boosting addresses this by constructing an ensemble of trees sequentially. At each step, a new tree is trained to predict the residual errors of the current ensemble. This iterative approach allows the classifier to improve gradually, with later trees focusing on events that were previously misclassified.

The algorithm minimizes a regularized objective function:
\begin{equation}
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{m=1}^{M} \Omega(f_m),
\end{equation}
where $n$ is the number of training events, $y_i \in \{0,1\}$ is the true class label of event $i$ (with $y_i = 1$ for signal and $y_i = 0$ for background), and $\hat{y}_i$ is the model's current prediction for that event. The loss function $l(y_i, \hat{y}_i)$ measures the prediction error—quantifying how far the predicted value $\hat{y}_i$ deviates from the true label $y_i$—while $\Omega(f_m)$ penalizes complex trees to prevent overfitting. Common choices for the loss function include logistic loss (also called log-loss or cross-entropy) for classification, which for a single event takes the form $l(y, \hat{y}) = -[y \log(\hat{y}) + (1-y)\log(1-\hat{y})]$, penalizing confident but incorrect predictions more heavily than uncertain ones. The regularization term is:
\begin{equation}
\Omega(f) = \gamma T + \tfrac{1}{2}\lambda \|\mathbf{w}\|^2 + \alpha \|\mathbf{w}\|_1,
\end{equation}
where $T$ is the number of leaves in the tree and $\mathbf{w} = (w_1, w_2, \ldots, w_T)$ is the vector of \textit{leaf weights}—the prediction values assigned to each terminal node within a single tree, conceptually analogous to the leaf probabilities described for Random Forests. However, in XGBoost these leaf weights are treated as learnable parameters that are explicitly optimized during training, rather than being fixed as simple class frequency ratios. The distinction in notation is important: $\mathbf{w}$ denotes weights \textit{within} individual trees (at the leaf level), while $\gamma_m$ in the ensemble expression $F_M(\mathbf{x}) = \sum_{m=1}^{M} \gamma_m h_m(\mathbf{x})$ denotes weights \textit{between} trees in the full ensemble. The regularization coefficients $\gamma$, $\lambda$, $\alpha$ control tree complexity: $\|\mathbf{w}\|^2$ is the $L^2$ norm (Euclidean norm) squared, defined as $\|\mathbf{w}\|^2 = \sum_{j=1}^{T} w_j^2$, which penalizes large leaf weights and encourages smoother predictions; $\|\mathbf{w}\|_1$ is the $L^1$ norm (Manhattan norm), defined as $\|\mathbf{w}\|_1 = \sum_{j=1}^{T} |w_j|$, which promotes sparsity by driving some leaf weights toward exactly zero. Together, these regularization terms prevent the model from assigning extreme values to individual leaves, which would indicate overfitting to training data noise.

The model is built additively through sequential updates:
\begin{equation}
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(\mathbf{x}_i),
\end{equation}
where $t = 1, 2, \ldots, M$ indexes the boosting iteration (tree number), $\hat{y}_i^{(t)}$ is the ensemble prediction for event $i$ after adding $t$ trees, and $\eta \in (0,1]$ is the \textit{learning rate} (or shrinkage parameter). The learning rate controls how much each new tree contributes to the ensemble: smaller values (e.g., $\eta = 0.01$–$0.1$) make the learning process more conservative, requiring more trees but typically improving generalization, while larger values (e.g., $\eta = 0.3$–$1.0$) lead to faster training but higher risk of overfitting. The dependence on $t$ is implicit: at iteration $t$, the new tree $f_t(\mathbf{x}_i)$ is trained to correct the residual errors of the current ensemble $\hat{y}_i^{(t-1)}$, so the contribution of each successive tree adapts to the evolving state of the model. Starting from $\hat{y}_i^{(0)} = 0$ (or a constant initial guess), each iteration adds a weighted correction: $\hat{y}_i^{(1)} = \eta f_1(\mathbf{x}_i)$, then $\hat{y}_i^{(2)} = \eta f_1(\mathbf{x}_i) + \eta f_2(\mathbf{x}_i)$, and so on, with later trees focusing on increasingly difficult cases.

Each new tree is selected to minimize a second-order Taylor approximation of the loss:
\begin{equation}
f_t = \arg\min_{f} \sum_{i=1}^{n} \left[g_i f(\mathbf{x}_i) + \tfrac{1}{2} h_i f^{(2)}(\mathbf{x}_i)\right] + \Omega(f),
\end{equation}
where $g_i$ and $h_i$ are the first and second derivatives of the loss function $l(y_i, \hat{y}_i)$ with respect to the current prediction $\hat{y}_i^{(t-1)}$, evaluated at event $i$:
\begin{align}
g_i &= \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}_i}\bigg|_{\hat{y}_i = \hat{y}_i^{(t-1)}}, \\
h_i &= \frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}_i^2}\bigg|_{\hat{y}_i = \hat{y}_i^{(t-1)}}.
\end{align}
These gradient statistics $g_i$ and $h_i$ depend on three quantities: the true label $y_i$ of event $i$, the current ensemble prediction $\hat{y}_i^{(t-1)}$ before adding tree $t$, and the specific form of the loss function. For the default logistic loss (binary cross-entropy) used in classification, $l(y, \hat{y}) = -[y \log(p) + (1-y)\log(1-p)]$ where $p = \sigma(\hat{y}) = 1/(1+e^{-\hat{y}})$ is the predicted probability obtained by applying the sigmoid function to the raw prediction $\hat{y}$. The derivatives are:
\begin{align}
g_i &= \frac{\partial l}{\partial \hat{y}_i} = p_i^{(t-1)} - y_i = \sigma(\hat{y}_i^{(t-1)}) - y_i, \\
h_i &= \frac{\partial^2 l}{\partial \hat{y}_i^2} = p_i^{(t-1)}(1 - p_i^{(t-1)}),
\end{align}
where $p_i^{(t-1)} = \sigma(\hat{y}_i^{(t-1)})$ is the current probability estimate. The first derivative $g_i$ represents the residual error: for a correctly classified signal event ($y_i = 1$) with high confidence ($p_i \approx 1$), $g_i \approx 0$ and the event receives little attention; for a misclassified signal event with low confidence ($p_i \approx 0$), $g_i \approx -1$ and the next tree focuses heavily on correcting this mistake. The second derivative $h_i$ acts as an adaptive weight: it is largest ($h_i = 0.25$) when the model is most uncertain ($p_i = 0.5$) and smallest near the decision boundaries ($p_i \to 0$ or $p_i \to 1$), effectively allocating more optimization effort to ambiguous cases.

The performance of XGBoost depends on finding the right balance between underfitting and overfitting. Underfitting occurs when the model is too simple to capture the relevant patterns in the data, leading to poor performance on both training and test sets. In this case the model is characterized by high bias and low variance. Overfitting, on the other hand, arises when the model learns the training data too closely, including statistical fluctuations and noise. This yields very good performance on the training set but poor generalization to new data, with the model showing low bias and high variance.

Several hyperparameters play a central role in controlling this balance:

\begin{itemize}
    \item \textbf{n\_estimators}: Number of boosting rounds. Too few trees lead to underfitting, while too many lead to overfitting. Early stopping is commonly used to determine the optimal number by monitoring validation performance.
    
    \item \textbf{Learning rate ($\eta$)}: Step size shrinkage applied at each boosting step. Small values (e.g.\ 0.01–0.1) improve generalization but require more trees; larger values speed up training but can overfit.
    
    \item \textbf{max\_depth}: Maximum depth of individual trees. Shallower trees (4–6) tend to be more stable, while deeper ones (7–10) can capture complex correlations but risk overfitting.
    
    \item \textbf{Regularization parameters}:
    \begin{itemize}
        \item $\gamma$: Minimum loss reduction required for a split. Larger values make the algorithm more conservative.
        \item $\lambda$: L2 regularization on leaf weights, which limits large values and stabilizes the model.
        \item $\alpha$: L1 regularization on leaf weights, which promotes sparsity and can serve as implicit feature selection.
    \end{itemize}
\end{itemize}
Optimal hyperparameters are typically found through systematic search methods. The most common approach is \textit{grid search with cross-validation} (GridSearchCV), which exhaustively tests all parameter combinations within predefined ranges. The core of this method is \textit{k-fold cross-validation}, a robust technique for assessing model generalization. 

The k-fold cross-validation procedure consists of the following steps: First, the available training data is randomly shuffled and partitioned into k equal-sized subsets called folds. This partitioning is typically stratified to preserve the class distribution in each fold. Then, the model is trained and evaluated k times in a round-robin fashion. For each iteration i (where i = 1 to k), the i-th fold is held out as validation data, while the remaining k-1 folds are used for training. The model's performance metric (typically negative log-loss, accuracy, or area under the ROC curve) is computed on the validation fold. After all k iterations are completed, the performance scores from each validation fold are averaged to produce a single estimation of the model's generalization error. This approach ensures that every data point is used exactly once for validation while being used k-1 times for training, providing an unbiased estimate of model performance that is robust to the specific partitioning of the data.


For XGBoost in HEP applications, we typically use k$=5$ folds as it offers a good balance between computational cost and reliable error estimation. Each parameter combination is evaluated through this cross-validation process, ensuring that selected parameters generalize well beyond the training data and are not overly tuned to specific statistical fluctuations.

The grid search tests all combinations in the parameter space defined by ranges such as: \texttt{learning\_rate} $\eta \in [0.01, 0.3]$, \texttt{max\_depth} $\in [3, 10]$, \texttt{n\_estimators} $\in [100, 1000]$, with regularization parameters $\gamma$, $\lambda$, and $\alpha$ typically explored in logarithmic scales.

While grid search is thorough, it becomes computationally expensive for high-dimensional parameter spaces. In such cases, more efficient methods like \textit{randomized search} (which samples parameter combinations randomly) or \textit{Bayesian optimization} (which uses probabilistic models to guide the search toward promising regions) can be employed. 

The optimization process is iterative: initial broad searches identify promising parameter regions, followed by finer-grained searches around the best-performing configurations. Early stopping during training—monitoring validation performance and halting when no improvement is observed for a specified number of rounds—prevents overfitting and significantly reduces computational cost, making the hyperparameter optimization feasible for large-scale HEP analyses.

\subsection{Standard ML Analysis Workflow}
\label{ssec:ml_workflow}

The XGBoost output score $f(\mathbf{x})$ transforms high-dimensional data into a single optimal discriminant. When binned, the resulting histogram gives expected yields:
\begin{align}
s_i &= \int_{\text{bin } i} \sigma_s \cdot \mathcal{L} \cdot \epsilon_s \cdot p_s(f) \, df, \\
b_i &= \int_{\text{bin } i} \sigma_b \cdot \mathcal{L} \cdot \epsilon_b \cdot p_b(f) \, df,
\end{align}
where $p_s(f)$ and $p_b(f)$ are the output distributions. 

Integrating machine learning into high-energy physics analysis follows a standardized workflow designed to maximize sensitivity while ensuring robustness against overfitting and systematic biases:

\begin{enumerate}
    \item \textbf{Dataset Preparation and Balancing}: Monte Carlo simulations generate signal and background samples. The signal sample corresponds to the hypothetical new physics process, while background samples include all known Standard Model processes that can produce similar experimental signatures. To prevent classifier bias toward the typically dominant background, datasets are balanced through undersampling (selecting a subset of the majority class) or, more commonly, event weighting using $w_i = \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}}$. Equal numbers of signal and background events are often used during training to ensure the algorithm learns both classes effectively, though the final evaluation uses proper physics weights.
    
    \item \textbf{Feature Preprocessing}: Input variables (kinematic observables such as $p_T$, $\eta$, $\phi$, invariant masses, and angular separations) are standardized using techniques like StandardScaler (transforming to zero mean and unit variance) or MinMaxScaler (scaling to a fixed range, typically [0, 1]). While tree-based methods like XGBoost are theoretically scale-insensitive, preprocessing improves numerical stability and convergence speed. Dimensionality reduction techniques like Principal Component Analysis (PCA) may be used for visualization or to address severe multicollinearity, though trees naturally handle correlated features.
    
    \item \textbf{Model Training and Hyperparameter Optimization}: The classifier is trained on the preprocessed data using the procedures described in Sec.~\ref{ssec:xgboost}. Key hyperparameters—including learning rate, maximum tree depth, L1/L2 regularization strengths, and minimum child weight—are optimized via grid search, random search, or Bayesian optimization as detailed in the hyperparameter optimization strategy. Performance is evaluated using k-fold cross-validation to ensure generalizability and avoid overfitting, with the optimal configuration selected based on the best cross-validated performance.
    
    \item \textbf{Output Score Generation}: Instead of binary class assignments, the trained model's continuous output is obtained using \texttt{predict\_proba()}, which provides a per-event probability score $f(\mathbf{x}) \in [0, 1]$ indicating the likelihood of belonging to the signal class. This score serves as a powerful discriminant variable that encapsulates the multidimensional separation power.
    
    \item \textbf{Histogram Construction and Weighting}: Events are binned based on their classifier score to form a one-dimensional histogram. Each bin's content is calculated using the appropriate physics-level weights:
    \begin{equation}
    N_i^{\text{bin}} = \sum_{j \in \text{events in bin } i} w_j = \sum_{j \in \text{events in bin } i} \left( \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}} \right)_j,
    \end{equation}
    yielding the expected signal ($s_i$) and background ($b_i$) yields per bin. The binning is typically optimized to maximize the expected sensitivity, often with finer binning in regions of better signal-to-background ratio.
    
    \item \textbf{Sensitivity Measurement}: The final histogram, incorporating all relevant systematic uncertainties as nuisance parameters, serves as input to the statistical model described in Sec.~\ref{sec:power_analysis}. The discovery significance $\kappa$ (from Eq.~\ref{eq:kappa_with_systematics}) is computed, quantifying the analysis sensitivity and enabling comparison between different analysis strategies or machine learning approaches.
\end{enumerate}

This end-to-end workflow seamlessly integrates machine learning into the established statistical framework of particle physics, transforming high-dimensional data into an optimized discriminant for sensitivity extraction.
