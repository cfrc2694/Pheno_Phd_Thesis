\section{ML enhanced signal-background discrimination}
\label{sec:machine_learning}

As shown in Section~\ref{sec:power_analysis}, the sensitivity of a search depends on optimally separating signal and background processes. Traditional cut-based analyses, which apply sequential selection criteria to individual observables, cannot fully exploit the discriminatory information contained in the high-dimensional feature space of collision events. In this approach, for an event described by a feature vector $\mathbf{x} = (x_1, x_2, \ldots, x_N)$, cuts are applied in the form
\begin{equation}
\text{Selection:} \quad x_1 > c_1 \,\text{AND}\, x_2 > c_2 \,\text{AND}\, \ldots \,\text{AND}\, x_N > c_N.
\end{equation}

This method has several limitations that become apparent when considering correlations among kinematic variables. A typical LHC event contains a large number of observables, and an optimal discriminator must consider these variables and their relationships simultaneously. Hard cut boundaries discard events that are signal-like in multivariate space but fall just outside univariate cuts. The challenge of dimensionality also arises, as optimizing many cuts becomes unstable and prone to statistical fluctuations. Finally, sequential cuts cannot capture non-linear relationships and complex decision boundaries that often provide the strongest discrimination.

Supervised learning addresses these limitations directly. It learns a function $f(\mathbf{x})$ that maps the high-dimensional input space to a continuous score approximating the posterior probability:
\begin{equation}
f: \mathbb{R}^N \rightarrow [0,1], \quad f(\mathbf{x}) \approx P(\text{signal} \mid \mathbf{x}).
\end{equation}
This score incorporates correlations and non-linearities present in the training data, providing a powerful, continuous discriminant.

Formally, the classification problem can be stated as follows. Each collision event is represented by a feature vector $\mathbf{x} = (x_1, x_2, \ldots, x_N)$, where the components correspond to reconstructed kinematic variables or high-level observables. The task is to assign a label
\begin{equation}
y =
\begin{cases}
1 & \text{if the event originates from the signal process}, \\
0 & \text{if the event originates from the background}.
\end{cases}
\end{equation}
Rather than predicting a hard label, modern classifiers return a continuous score $f(\mathbf{x}) \in [0,1]$ that estimates the probability of an event being signal given its features. This formulation allows the classifier to exploit multidimensional correlations and complex decision boundaries that cut-based methods cannot capture. In high-energy physics, the objective is not conventional metrics such as accuracy or precision, but rather the improvement of search sensitivity. The classifier serves as an intermediate tool: it provides a discriminant that maximizes the separation of signal and background distributions, which is then used in hypothesis testing and limit setting.

Several algorithms are commonly employed in this context. Logistic regression is often used as a baseline due to its simplicity and transparency. It assumes a linear decision boundary in the feature space, with the discriminant
\begin{equation}
f(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b), \quad 
\sigma(z) = \frac{1}{1 + e^{-z}},
\end{equation}
where $\mathbf{w}$ are model parameters and $b$ is a bias term. While it cannot capture complex non-linear relationships, logistic regression remains useful when signal and background are approximately linearly separable, and it provides a clear reference point against which more sophisticated methods can be compared.

Support Vector Machines (SVMs) instead seek to maximize the margin between signal and background classes. For non-linear problems, SVMs employ kernel functions $K(\mathbf{x}_i, \mathbf{x})$ that implicitly map the inputs to a higher-dimensional space where linear separation becomes possible. The decision function is
\begin{equation}
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right),
\end{equation}
where $\alpha_i$ are Lagrange multipliers. SVMs are effective in high-dimensional spaces and have strong theoretical guarantees, but they scale poorly to large datasets and are highly sensitive to kernel choice and hyperparameter tuning.

Tree-based ensembles are among the most widely used methods. Random Forests combine multiple decision trees trained on bootstrap samples of the data, with random feature selection at each split:
\begin{equation}
F(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} T_b(\mathbf{x}),
\end{equation}
where $B$ is the number of trees and $T_b$ is the prediction of the $b$-th tree. This approach reduces variance and mitigates overfitting through averaging. Random Forests are robust, parallelizable, and provide natural feature importance measures, though they typically require more trees than boosted methods and may yield slightly lower performance on well-tuned tasks.

Boosted Decision Trees (BDTs) have become a standard tool in particle physics because they balance interpretability and performance. A single decision tree partitions the feature space through binary splits (e.g., ``Is $x_i < \text{threshold}$?'') and assigns class probabilities to terminal nodes. On their own, trees are high-variance learners, but boosting combines many weak learners (typically shallow trees) into a strong ensemble:
\begin{equation}
F_M(\mathbf{x}) = \sum_{m=1}^{M} \gamma_m h_m(\mathbf{x}),
\end{equation}
where each new tree $h_m(\mathbf{x})$ corrects the errors of the current ensemble $F_{m-1}(\mathbf{x})$, and $\gamma_m$ is its weight. This sequential correction process produces a powerful classifier. BDTs are robust to outliers and non-Gaussian distributions, handle mixed variable types naturally, and perform implicit feature selection, often revealing which observables are most discriminating. These properties explain their widespread adoption in LHC analyses.

Finally, Deep Neural Networks (DNNs) represent the most expressive class of models, capable of learning highly complex, hierarchical representations of the input data. A deep network with multiple hidden layers can be written as
\begin{equation}
f(\mathbf{x}) = \sigma^{(L)}\left(W^{(L)} \sigma^{(L-1)}\left( \cdots \sigma^{(1)}(W^{(1)} \mathbf{x} + b^{(1)}) \cdots \right) + b^{(L)}\right),
\end{equation}
where $L$ is the number of layers. DNNs excel at capturing intricate patterns and correlations, but they require large training datasets, careful regularization, and extensive hyperparameter tuning. Their black-box nature also complicates interpretability and the assessment of systematic uncertainties.

In practice, Random Forests are often used as robust, low-maintenance alternatives, while SVMs have largely been superseded by tree-based methods and neural networks due to scalability issues. DNNs can outperform other methods on very complex problems with sufficient data, but they demand significantly more computational resources. For most LHC searches, BDTs—particularly implementations such as XGBoost—provide the best balance between performance, interpretability, and computational efficiency. In Sec.~\ref{sec:ML}, and in Table~\ref{tab:ml_results}, we compare the performance of different methods and show that the gain in accuracy from DNNs is marginal compared to BDTs, while the training time is substantially larger.


\subsection{XGBoost: Optimized Gradient Boosting}
\label{ssec:xgboost}

\textit{XGBoost} (eXtreme Gradient Boosting) is a widely used implementation of gradient boosting that has become standard in high-energy physics due to its efficiency, predictive power, and robustness. 

A single decision tree is a weak classifier: it partitions the feature space through binary splits but is highly sensitive to fluctuations in the training data. Boosting addresses this by constructing an ensemble of trees sequentially. At each step, a new tree is trained to predict the residual errors of the current ensemble. This iterative approach allows the classifier to improve gradually, with later trees focusing on events that were previously misclassified.

The algorithm minimizes a regularized objective function:
\begin{equation}
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{m=1}^{M} \Omega(f_m),
\end{equation}
where $l(y_i, \hat{y}_i)$ measures the prediction error and $\Omega(f_m)$ penalizes complex trees to prevent overfitting. The regularization term is:
\begin{equation}
\Omega(f) = \gamma T + \tfrac{1}{2}\lambda \|\mathbf{w}\|^2 + \alpha \|\mathbf{w}\|_1,
\end{equation}
where $T$ is the number of leaves, $\mathbf{w}$ are the leaf weights, and $\gamma$, $\lambda$, $\alpha$ control tree complexity.

The model is built additively:
\begin{equation}
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(\mathbf{x}_i),
\end{equation}
where $\eta$ is the learning rate. Each new tree is selected to minimize:
\begin{equation}
f_t = \arg\min_{f} \sum_{i=1}^{n} \left[g_i f(\mathbf{x}_i) + \tfrac{1}{2} h_i f^2(\mathbf{x}_i)\right] + \Omega(f),
\end{equation}
with $g_i$ and $h_i$ the first and second derivatives of the loss function.

\paragraph{Overfitting, Underfitting, and Hyperparameter Tuning}

The performance of XGBoost depends on finding the right balance between underfitting and overfitting:

\begin{itemize}
    \item \textbf{Underfitting} occurs when the model is too simple to capture the underlying patterns in the data. This results in poor performance on both training and test data. Signs of underfitting include high bias and low variance.
    
    \item \textbf{Overfitting} occurs when the model learns the training data too well, including its noise and fluctuations. This results in excellent training performance but poor generalization to new data. Signs of overfitting include low bias and high variance.
\end{itemize}

The key hyperparameters that control this balance are:

\begin{itemize}
    \item \textbf{n\_estimators}: The number of boosting rounds. Too few trees underfit, while too many overfit. We use early stopping to determine the optimal number automatically by monitoring performance on a validation set.
    
    \item \textbf{Learning rate ($\eta$)}: Controls the step size shrinkage. Smaller values (0.01–0.1) require more trees but generalize better. Larger values (0.2–0.3) learn faster but may overfit.
    
    \item \textbf{max\_depth}: The maximum tree depth. Shallower trees (4–6) are more robust, deeper trees (7–10) can capture complex patterns but risk overfitting.
    
    \item \textbf{Regularization parameters}:
    \begin{itemize}
        \item $\gamma$: Minimum loss reduction for a split. Higher values (1–5) prevent overfitting by making splits more conservative.
        \item $\lambda$: L2 regularization on weights. Values of 1–10 help control large leaf values.
        \item $\alpha$: L1 regularization on weights. Values of 0–1 promote sparsity.
    \end{itemize}
    
    \item \textbf{Subsampling parameters}:
    \begin{itemize}
        \item \texttt{subsample}: Fraction of events used per tree (0.6–0.9)
        \item \texttt{colsample\_bytree}: Fraction of features used per tree (0.6–0.9)
    \end{itemize}
    These introduce randomness that improves generalization.
\end{itemize}

\paragraph{Hyperparameter Optimization Strategy}

We use a systematic approach to find optimal hyperparameters:

\begin{enumerate}
    \item \textbf{Define search space}: Establish reasonable ranges for each parameter based on physics requirements and computational constraints.
    
    \item \textbf{Cross-validation}: Use k-fold cross-validation (typically k=5) to assess generalization performance for each parameter combination.
    
    \item \textbf{Grid search or Bayesian optimization}: Methodically explore the parameter space. Grid search tests all combinations in a predefined grid, while Bayesian optimization uses probabilistic models to guide the search more efficiently.
    
    \item \textbf{Early stopping}: Monitor validation performance during training and stop when no improvement is observed for a specified number of rounds.
    
    \item \textbf{Final evaluation}: Test the best configuration on an independent test set to verify generalization performance.
\end{enumerate}

The optimal configuration is typically found through iterative refinement, starting with broader searches and progressively narrowing the parameter ranges based on performance.

\paragraph{Physics-Specific Considerations}

Several XGBoost features are particularly valuable for HEP applications:

\begin{itemize}
    \item \textbf{Event weights}: Direct support for physics weights ($w_i = \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}}$) ensures the classifier optimizes for expected yields.
    
    \item \textbf{Early stopping}: Prevents overfitting and saves computational resources.
    
    \item \textbf{Missing value handling}: Learns optimal defaults for missing features, useful for detector coverage issues.
    
    \item \textbf{Feature importance}: Provides quantitative measures of which variables contribute most to discrimination.
\end{itemize}

\paragraph{Connection to Statistical Significance}

The XGBoost output score $f(\mathbf{x})$ transforms high-dimensional data into a single optimal discriminant. When binned, the resulting histogram gives expected yields:
\begin{align}
s_i &= \int_{\text{bin } i} \sigma_s \cdot \mathcal{L} \cdot \epsilon_s \cdot p_s(f) \, df, \\
b_i &= \int_{\text{bin } i} \sigma_b \cdot \mathcal{L} \cdot \epsilon_b \cdot p_b(f) \, df,
\end{align}
where $p_s(f)$ and $p_b(f)$ are the output distributions. A well-tuned model maximizes the separation between these distributions, thereby maximizing the discovery significance $\kappa$ when Neyman-Pearson weights are applied. This automated optimization across multiple observables achieves sensitivity unattainable with manual methods.


\subsection{Standard ML Analysis Workflow}
\label{ssec:ml_workflow}

Integrating machine learning into high-energy physics analysis follows a standardized workflow designed to maximize sensitivity while ensuring robustness against overfitting and systematic biases:

\begin{enumerate}
    \item \textbf{Dataset Preparation and Balancing}: Monte Carlo simulations generate signal and background samples. To prevent classifier bias toward the typically dominant background, datasets are balanced through undersampling (selecting subset of majority class) or event weighting ($w_i = \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}}$). Equal signal and background events are often used for training to ensure the algorithm learns both classes.
    
    \item \textbf{Feature Preprocessing}: Input variables are standardized using techniques like StandardScaler (zero mean, unit variance) or MinMaxScaler (fixed range). While tree-based methods like XGBoost are scale-insensitive, preprocessing aids convergence and interpretability. Dimensionality reduction techniques like PCA may be used for visualization or to address multicollinearity, though trees handle correlated features naturally.
    
    \item \textbf{Model Training and Hyperparameter Optimization}: The classifier is trained on preprocessed data. Key hyperparameters—learning rate, maximum tree depth, L1/L2 regularization terms, and minimum child weight—are optimized via grid search, random search, or Bayesian optimization. Performance is evaluated using k-fold cross-validation to ensure generalizability and avoid overfitting.
    
    \item \textbf{Output Score Generation}: Instead of hard class assignments, the trained model's continuous output is obtained using \texttt{predict\_proba()}, providing a per-event probability score $f(\mathbf{x}) \in [0, 1]$ for signal classification. This score serves as a powerful discriminant variable.
    
    \item \textbf{Histogram Construction and Weighting}: Events are binned by classifier score to form a histogram. Each bin's content uses physics-level weights:
    \[
    N_i^{\text{bin}} = \sum_{\text{events in bin } i} w_j = \sum_{\text{events in bin } i} \left( \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}} \right)_j,
    \]
    giving expected signal ($s_i$) and background ($b_i$) yields per bin.
    
    \item \textbf{Sensitivity Measurement}: The final histogram, with systematic uncertainties as nuisance parameters, feeds into the statistical model from Section~\ref{sec:power_analysis}. The discovery significance $\kappa$ (Eq.~\ref{eq:kappa_with_systematics}) is computed, quantifying analysis sensitivity and enabling strategy comparisons.
\end{enumerate}

This workflow integrates machine learning into particle physics' statistical framework, transforming high-dimensional data into an optimized discriminant for sensitivity extraction.


Tree-based methods allow for physics interpretation through feature importance analysis. Metrics like Gain (average purity improvement from splits), Cover (number of affected samples), and Frequency (usage count) rank the most discriminative observables. Advanced techniques like SHAP (SHapley Additive exPlanations) values show each feature's contribution to individual predictions. This helps validate that models use physically meaningful variables and guides future analysis strategies, detector calibration priorities, and experiment design.
