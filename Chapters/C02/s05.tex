\section{ML enhanced signal-background discrimination}
\label{sec:machine_learning}

As shown in Section~\ref{sec:power_analysis}, the sensitivity of a search depends on optimally separating signal and background processes~\cite{Cowan:2011,Guest2018}. Traditional cut-based analyses, which apply sequential selection criteria to individual observables, cannot fully exploit the discriminatory information contained in the high-dimensional feature space of collision events~\cite{Hastie2009,Guest2018}. In this approach, for an event described by a feature vector $\mathbf{x} = (x_1, x_2, \ldots, x_N)$, cuts are applied in the form~\cite{Hoecker2007}
\begin{equation}
\text{Selection:} \quad x_1 > c_1 \,\text{AND}\, x_2 > c_2 \,\text{AND}\, \ldots \,\text{AND}\, x_N > c_N.
\end{equation}

This method has several limitations that become apparent when considering correlations among kinematic variables~\cite{Hastie2009,Guest2018}. A typical LHC event contains a large number of observables, and an optimal discriminator must consider these variables and their relationships simultaneously~\cite{Hoecker2007,pedregosa_scikit-learn_2011}. Hard cut boundaries discard events that are signal-like in multivariate space but fall just outside univariate cuts~\cite{Hastie2009,Guest2018}. The challenge of dimensionality also arises, as optimizing many cuts becomes unstable and prone to statistical fluctuations~\cite{Hastie2009,friedman_greedy_2001}. Finally, sequential cuts cannot capture non-linear relationships and complex decision boundaries that often provide the strongest discrimination~\cite{Hastie2009,Guest2018}.

Supervised learning addresses these limitations directly~\cite{Hastie2009,Guest2018}. It learns a function $f(\mathbf{x})$ that maps the high-dimensional input space to a continuous score approximating the posterior probability~\cite{Cranmer2015,Platt1999}:
\begin{equation}
f: \mathbb{R}^N \rightarrow [0,1], \quad f(\mathbf{x}) \approx P(\text{signal} \mid \mathbf{x}).
\end{equation}
This score incorporates correlations and non-linearities present in the training data, providing a powerful, continuous discriminant~\cite{Hastie2009,Guest2018}.

Formally, the classification problem can be stated as follows~\cite{Hastie2009}. Each collision event is represented by a feature vector $\mathbf{x} = (x_1, x_2, \ldots, x_N)$, where the components correspond to reconstructed kinematic variables or high-level observables~\cite{CMS:PF2017,pedregosa_scikit-learn_2011}. The task is to assign a label
\begin{equation}
y =
\begin{cases}
1 & \text{if the event originates from the signal process}, \\
0 & \text{if the event originates from the background}.
\end{cases}
\end{equation}
Rather than predicting a hard label, modern classifiers return a continuous score $f(\mathbf{x}) \in [0,1]$ that estimates the probability of an event being signal given its features. This formulation allows the classifier to exploit multidimensional correlations and complex decision boundaries that cut-based methods cannot capture~\cite{Cranmer2015,Guest2018}.

Logistic regression, SVMs~\cite{Cortes1995}, Random Forests~\cite{Breiman2001} and boosted trees~\cite{friedman_greedy_2001,chen_xgboost_2016} are common choices; see \cite{Hastie2009,Hoecker2007} for comparisons and practical guidance.

Several algorithms are commonly employed in this context. Logistic regression is often used as a baseline due to its simplicity and transparency. It assumes a linear decision boundary in the feature space, with the discriminant
\begin{equation}
f(\mathbf{x}) = \sigma(\mathbf{w} \cdot \mathbf{x} + b), \quad 
\sigma(z) = \frac{1}{1 + e^{-z}},
\end{equation}
where $\mathbf{w}$ are model parameters and $b$ is a bias term. While it cannot capture complex non-linear relationships, logistic regression remains useful when signal and background are approximately linearly separable, and it provides a clear reference point against which more sophisticated methods can be compared.

Support Vector Machines (SVMs) instead seek to maximize the margin between signal and background classes. For non-linear problems, SVMs employ kernel functions $K(\mathbf{x}_i, \mathbf{x})$ that implicitly map the inputs to a higher-dimensional space where linear separation becomes possible. The decision function is
\begin{equation}
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b\right),
\end{equation}
where $\alpha_i$ are Lagrange multipliers. SVMs are effective in high-dimensional spaces and have strong theoretical guarantees, but they scale poorly to large datasets and are highly sensitive to kernel choice and hyperparameter tuning.

Tree-based ensembles are among the most widely used methods. Random Forests combine multiple decision trees trained on bootstrap samples of the data, with random feature selection at each split:
\begin{equation}
F(\mathbf{x}) = \frac{1}{B} \sum_{b=1}^{B} T_b(\mathbf{x}),
\end{equation}
where $B$ is the number of trees and $T_b$ is the prediction of the $b$-th tree. This approach reduces variance and mitigates overfitting through averaging. Random Forests are robust, parallelizable, and provide natural feature importance measures, though they typically require more trees than boosted methods and may yield slightly lower performance on well-tuned tasks.

Boosted Decision Trees (BDTs) have become a standard tool in particle physics because they balance interpretability and performance. A single decision tree partitions the feature space through binary splits (e.g., ``Is $x_i < \text{threshold}$?'') and assigns class probabilities to terminal nodes. On their own, trees are high-variance learners, but boosting combines many weak learners (typically shallow trees) into a strong ensemble:
\begin{equation}
F_M(\mathbf{x}) = \sum_{m=1}^{M} \gamma_m h_m(\mathbf{x}),
\end{equation}
where each new tree $h_m(\mathbf{x})$ corrects the errors of the current ensemble $F_{m-1}(\mathbf{x})$, and $\gamma_m$ is its weight. This sequential correction process produces a powerful classifier. BDTs are robust to outliers and non-Gaussian distributions, handle mixed variable types naturally, and perform implicit feature selection, often revealing which observables are most discriminating. These properties explain their widespread adoption in LHC analyses.

Finally, Deep Neural Networks (DNNs) represent the most expressive class of models, capable of learning highly complex, hierarchical representations of the input data. A deep network with multiple hidden layers can be written as
\begin{equation}
f(\mathbf{x}) = \sigma^{(L)}\left(W^{(L)} \sigma^{(L-1)}\left( \cdots \sigma^{(1)}(W^{(1)} \mathbf{x} + b^{(1)}) \cdots \right) + b^{(L)}\right),
\end{equation}
where $L$ is the number of layers. DNNs excel at capturing intricate patterns and correlations, but they require large training datasets, careful regularization, and extensive hyperparameter tuning. Their black-box nature also complicates interpretability and the assessment of systematic uncertainties.

In practice, Random Forests are often used as robust, low-maintenance alternatives, while SVMs have largely been superseded by tree-based methods and neural networks due to scalability issues. DNNs can outperform other methods on very complex problems with sufficient data, but they demand significantly more computational resources. For most LHC searches, BDTs—particularly implementations such as XGBoost—provide the best balance between performance, interpretability, and computational efficiency. In Sec.~\ref{sec:ML}, and in Table~\ref{tab:ml_results}, we compare the performance of different methods and show that the gain in accuracy from DNNs is marginal compared to BDTs, while the training time is substantially larger.


\subsection{XGBoost: Optimized Gradient Boosting}
\label{ssec:xgboost}

\textit{XGBoost} (eXtreme Gradient Boosting) is a widely used implementation of gradient boosting that has become standard in high-energy physics due to its efficiency, predictive power, and robustness. 

A single decision tree is a weak classifier: it partitions the feature space through binary splits but is highly sensitive to fluctuations in the training data. Boosting addresses this by constructing an ensemble of trees sequentially. At each step, a new tree is trained to predict the residual errors of the current ensemble. This iterative approach allows the classifier to improve gradually, with later trees focusing on events that were previously misclassified.

The algorithm minimizes a regularized objective function:
\begin{equation}
\mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{m=1}^{M} \Omega(f_m),
\end{equation}
where $l(y_i, \hat{y}_i)$ measures the prediction error and $\Omega(f_m)$ penalizes complex trees to prevent overfitting. The regularization term is:
\begin{equation}
\Omega(f) = \gamma T + \tfrac{1}{2}\lambda \|\mathbf{w}\|^2 + \alpha \|\mathbf{w}\|_1,
\end{equation}
where $T$ is the number of leaves, $\mathbf{w}$ are the leaf weights, and $\gamma$, $\lambda$, $\alpha$ control tree complexity.

The model is built additively:
\begin{equation}
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(\mathbf{x}_i),
\end{equation}
where $\eta$ is the learning rate. Each new tree is selected to minimize:
\begin{equation}
f_t = \arg\min_{f} \sum_{i=1}^{n} \left[g_i f(\mathbf{x}_i) + \tfrac{1}{2} h_i f^2(\mathbf{x}_i)\right] + \Omega(f),
\end{equation}
with $g_i$ and $h_i$ the first and second derivatives of the loss function.

The performance of XGBoost depends on finding the right balance between underfitting and overfitting. Underfitting occurs when the model is too simple to capture the relevant patterns in the data, leading to poor performance on both training and test sets. In this case the model is characterized by high bias and low variance. Overfitting, on the other hand, arises when the model learns the training data too closely, including statistical fluctuations and noise. This yields very good performance on the training set but poor generalization to new data, with the model showing low bias and high variance.

Several hyperparameters play a central role in controlling this balance:

\begin{itemize}
    \item \textbf{n\_estimators}: Number of boosting rounds. Too few trees lead to underfitting, while too many lead to overfitting. Early stopping is commonly used to determine the optimal number by monitoring validation performance.
    
    \item \textbf{Learning rate ($\eta$)}: Step size shrinkage applied at each boosting step. Small values (e.g.\ 0.01–0.1) improve generalization but require more trees; larger values speed up training but can overfit.
    
    \item \textbf{max\_depth}: Maximum depth of individual trees. Shallower trees (4–6) tend to be more stable, while deeper ones (7–10) can capture complex correlations but risk overfitting.
    
    \item \textbf{Regularization parameters}:
    \begin{itemize}
        \item $\gamma$: Minimum loss reduction required for a split. Larger values make the algorithm more conservative.
        \item $\lambda$: L2 regularization on leaf weights, which limits large values and stabilizes the model.
        \item $\alpha$: L1 regularization on leaf weights, which promotes sparsity and can serve as implicit feature selection.
    \end{itemize}
\end{itemize}
Optimal hyperparameters are typically found through systematic search methods. The most common approach is \textit{grid search with cross-validation} (GridSearchCV), which exhaustively tests all parameter combinations within predefined ranges. The core of this method is \textit{k-fold cross-validation}, a robust technique for assessing model generalization. 

The k-fold cross-validation procedure consists of the following steps: First, the available training data is randomly shuffled and partitioned into k equal-sized subsets called folds. This partitioning is typically stratified to preserve the class distribution in each fold. Then, the model is trained and evaluated k times in a round-robin fashion. For each iteration i (where i = 1 to k), the i-th fold is held out as validation data, while the remaining k-1 folds are used for training. The model's performance metric (typically negative log-loss, accuracy, or area under the ROC curve) is computed on the validation fold. After all k iterations are completed, the performance scores from each validation fold are averaged to produce a single estimation of the model's generalization error. This approach ensures that every data point is used exactly once for validation while being used k-1 times for training, providing an unbiased estimate of model performance that is robust to the specific partitioning of the data.


For XGBoost in HEP applications, we typically use k$=5$ folds as it offers a good balance between computational cost and reliable error estimation. Each parameter combination is evaluated through this cross-validation process, ensuring that selected parameters generalize well beyond the training data and are not overly tuned to specific statistical fluctuations.

The grid search tests all combinations in the parameter space defined by ranges such as: \texttt{learning\_rate} $\eta \in [0.01, 0.3]$, \texttt{max\_depth} $\in [3, 10]$, \texttt{n\_estimators} $\in [100, 1000]$, with regularization parameters $\gamma$, $\lambda$, and $\alpha$ typically explored in logarithmic scales.

While grid search is thorough, it becomes computationally expensive for high-dimensional parameter spaces. In such cases, more efficient methods like \textit{randomized search} (which samples parameter combinations randomly) or \textit{Bayesian optimization} (which uses probabilistic models to guide the search toward promising regions) can be employed. 

The optimization process is iterative: initial broad searches identify promising parameter regions, followed by finer-grained searches around the best-performing configurations. Early stopping during training—monitoring validation performance and halting when no improvement is observed for a specified number of rounds—prevents overfitting and significantly reduces computational cost, making the hyperparameter optimization feasible for large-scale HEP analyses.

\subsection{Standard ML Analysis Workflow}
\label{ssec:ml_workflow}

The XGBoost output score $f(\mathbf{x})$ transforms high-dimensional data into a single optimal discriminant. When binned, the resulting histogram gives expected yields:
\begin{align}
s_i &= \int_{\text{bin } i} \sigma_s \cdot \mathcal{L} \cdot \epsilon_s \cdot p_s(f) \, df, \\
b_i &= \int_{\text{bin } i} \sigma_b \cdot \mathcal{L} \cdot \epsilon_b \cdot p_b(f) \, df,
\end{align}
where $p_s(f)$ and $p_b(f)$ are the output distributions. 

Integrating machine learning into high-energy physics analysis follows a standardized workflow designed to maximize sensitivity while ensuring robustness against overfitting and systematic biases:

\begin{enumerate}
    \item \textbf{Dataset Preparation and Balancing}: Monte Carlo simulations generate signal and background samples. The signal sample corresponds to the hypothetical new physics process, while background samples include all known Standard Model processes that can produce similar experimental signatures. To prevent classifier bias toward the typically dominant background, datasets are balanced through undersampling (selecting a subset of the majority class) or, more commonly, event weighting using $w_i = \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}}$. Equal numbers of signal and background events are often used during training to ensure the algorithm learns both classes effectively, though the final evaluation uses proper physics weights.
    
    \item \textbf{Feature Preprocessing}: Input variables (kinematic observables such as $p_T$, $\eta$, $\phi$, invariant masses, and angular separations) are standardized using techniques like StandardScaler (transforming to zero mean and unit variance) or MinMaxScaler (scaling to a fixed range, typically [0, 1]). While tree-based methods like XGBoost are theoretically scale-insensitive, preprocessing improves numerical stability and convergence speed. Dimensionality reduction techniques like Principal Component Analysis (PCA) may be used for visualization or to address severe multicollinearity, though trees naturally handle correlated features.
    
    \item \textbf{Model Training and Hyperparameter Optimization}: The classifier is trained on the preprocessed data using the procedures described in Section~\ref{ssec:xgboost}. Key hyperparameters—including learning rate, maximum tree depth, L1/L2 regularization strengths, and minimum child weight—are optimized via grid search, random search, or Bayesian optimization as detailed in the hyperparameter optimization strategy. Performance is evaluated using k-fold cross-validation to ensure generalizability and avoid overfitting, with the optimal configuration selected based on the best cross-validated performance.
    
    \item \textbf{Output Score Generation}: Instead of binary class assignments, the trained model's continuous output is obtained using \texttt{predict\_proba()}, which provides a per-event probability score $f(\mathbf{x}) \in [0, 1]$ indicating the likelihood of belonging to the signal class. This score serves as a powerful discriminant variable that encapsulates the multidimensional separation power.
    
    \item \textbf{Histogram Construction and Weighting}: Events are binned based on their classifier score to form a one-dimensional histogram. Each bin's content is calculated using the appropriate physics-level weights:
    \[
    N_i^{\text{bin}} = \sum_{\text{events in bin } i} w_j = \sum_{\text{events in bin } i} \left( \sigma \cdot \mathcal{L} \cdot \epsilon / N_{\text{gen}} \right)_j,
    \]
    yielding the expected signal ($s_i$) and background ($b_i$) yields per bin. The binning is typically optimized to maximize the expected sensitivity, often with finer binning in regions of better signal-to-background ratio.
    
    \item \textbf{Sensitivity Measurement}: The final histogram, incorporating all relevant systematic uncertainties as nuisance parameters, serves as input to the statistical model described in Section~\ref{sec:power_analysis}. The discovery significance $\kappa$ (from Eq.~\ref{eq:kappa_with_systematics}) is computed, quantifying the analysis sensitivity and enabling comparison between different analysis strategies or machine learning approaches.
\end{enumerate}

This end-to-end workflow seamlessly integrates machine learning into the established statistical framework of particle physics, transforming high-dimensional data into an optimized discriminant for sensitivity extraction.
