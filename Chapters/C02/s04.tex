\section{Measurement of the Power of an Analysis}
\label{sec:power_analysis}

In high-energy physics experiments, data is often discretized into bins (e.g., histograms of collision events versus energy or momentum) to test competing hypotheses. The fundamental framework compares two scenarios: the \textit{null hypothesis} ($H_0$), representing background-only processes ($b_i$ in each bin $i$), and the \textit{alternative hypothesis} ($H_1$), including both signal and background ($s_i + b_i$). Given the Poissonian nature of event counts $n_i$, the likelihood for observing the data under each hypothesis is:
\begin{equation}
    \mathcal{L}(n_i \mid \lambda_i) = \frac{e^{-\lambda_i} \lambda_i^{n_i}}{n_i!}, \quad \text{where } \lambda_i = 
    \begin{cases}
        b_i & \text{for } H_0, \\
        s_i + b_i & \text{for } H_1.
    \end{cases}
\end{equation}
The Neyman-Pearson lemma~\parencite{NeymanPearson1933} provides a rigorous framework for hypothesis testing by establishing that the \textit{likelihood ratio} $Q = \mathcal{L}(\text{data} \mid H_1)/\mathcal{L}(\text{data} \mid H_0)$ is the most powerful test statistic for distinguishing between two simple hypotheses, $H_0$ and $H_1$. This forms the basis for quantifying the evidence for new physics signals against known backgrounds. For binned analyses in particle physics, we define the likelihood ratio $Q_i$ for each bin $i$ as,
\begin{equation}
Q_i = \frac{\mathcal{L}(n_i \mid s_i + b_i)}{\mathcal{L}(n_i \mid b_i)} = e^{-s_i} \left( 1+\frac{s_i}{b_i} \right)^{n_i},
\end{equation}
where $n_i$ is the observed event count, $s_i$ the expected signal, and $b_i$ the expected background in bin $i$. 

The test for the full analysis is constructed as the product of individual bin likelihood ratios:
\begin{equation}
Q = \prod_{i=1}^{N} Q_i,
\end{equation}
where $N$ is the total number of bins. Under this formulation, each bin is treated as an independent experiment, allowing us to analyze the data in a modular way. This is particularly useful when combining results from multiple search channels or energy ranges. 

For convenience and in order to connect with asymptotic distributions, we take the logarithm of the likelihood ratio, turning the product into a sum:
\begin{equation}
-2\ln Q = 2\sum_{i=1}^{N}\left[s_i - n_i \ln\left(1 + \frac{s_i}{b_i}\right)\right],
\end{equation}
where $n_i$ is the observed number of events, $b_i$ the expected background, and $s_i$ the expected signal in bin $i$. By Wilks' theorem~\parencite{Wilks1938}, the asymptotic distribution of this test statistic under the background-only hypothesis ($H_0$) follows a $\chi^2$ distribution, facilitating $p$-value calculations and hypothesis testing.

In practice, the Neyman-Pearson lemma motivates the use of a test statistic $t$ that quantifies the evidence for a signal against the background-only hypothesis, which can be written as
\begin{equation}
t=-2\ln Q = \sum_{i=1}^{N} \left[2s_i - 2n_i w_i\right],
\end{equation}
with the optimal weight of each bin given by $w_i = \ln\!\left(1 + \frac{s_i}{b_i}\right)$.




The discovery significance $\kappa$ quantifies the statistical separation of $t$ if $n$ is distributed according to the background-only hypothesis ($H_0$) versus the signal-plus-background hypothesis ($H_1$), normalized by the standard deviation of the $H_1$ distribution,
\begin{equation}
\kappa = \frac{\braket{t}_{H_0} - \braket{t}_{H_1}}{\sigma_{H_1}}.
\end{equation}
The expected behavior differs under the signal-plus-background ($H_1$) and background-only ($H_0$) hypotheses:

\begin{itemize}
	\item \textbf{Under $H_1$} we expect that the $n_i$ data distribution follows $\sim \text{Pois}(s_i + b_i)$:
	\begin{equation}
	\langle -2\ln Q \rangle_{s+b} = \sum_i \left[2s_i - 2(s_i + b_i)w_i\right]
	\implies \sigma^2_{s+b} = 4\sum_i (s_i + b_i) w_i^2.
	\end{equation}

	\item \textbf{Under $H_0$} we expect that the $n_i$ data distribution follows $\text{Pois}(b_i)$
	\begin{equation}
	\langle -2\ln Q \rangle_{b} = \sum_i \left[2s_i - 2b_i w_i\right]
	\implies \sigma^2_{b} = 4\sum_i b_i w_i^2
	\end{equation}
\end{itemize}
Substituting in $\kappa$ gives a useful expression for the discovery significance,
\begin{align}
\kappa = \frac{\sum s_i w_i}{\sqrt{\sum (s_i + b_i) w_i^2}}
\end{align}
It quantifies the separation between the signal+background ($s+b$) and background-only hypotheses in units of standard deviations ($\sigma$), where $\kappa = 5$ corresponds to the traditional $5\sigma$ discovery threshold, $\kappa =3$ to a $3\sigma$ evidence to the traditional anomaly detection threshold, and $\kappa = 1.69$ to the $95\%$ confidence level (CL) exclusion limit.


This figure of merit automatically optimizes sensitivity through the logarithmic weights $w_i = \ln(1 + s_i/b_i)$, which naturally emphasize bins with either high signal-to-background ratios ($s_i/b_i$) or large absolute signal contributions ($s_i$). In asymptotic limits, $\kappa$ simplifies to intuitive forms: for dominant signals ($s_i \gg b_i$), it approaches $\sqrt{\sum s_i}$ (Poisson counting), while in background-dominated regimes ($s_i \ll b_i$), it reduces to an inverse-variance-weighted sum $\sum s_i / \sqrt{\sum b_i (s_i/b_i)^2}$. This dual behavior ensures optimal discrimination power across all signal regimes.

In practice, we must take into account systematic effects by incorporating nuisance parameters into the likelihood and profiling over uncertainty ranges. The power calculation can be extended to include systematic uncertainties by modifying the denominator as,
\begin{equation}
	\boxed{
	\kappa_{\text{sys}} = \frac{\sum_i s_i w_i}{\sqrt{\sum_i \left[(s_i + b_i) + \delta^2_{\text{sys,signal},i} + \delta^2_{\text{sys,bkg},i}\right] w_i^2}},
}
\label{eq:kappa_with_systematics}
\end{equation}
where $\delta_{\text{sys}}$ terms represent the systematic uncertainties on signal and background predictions.
