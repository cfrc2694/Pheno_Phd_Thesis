\section{Phenomenological Pipeline}
\label{sec:pheno_pipeline}

The estimation of signal and background event yields is performed through a comprehensive Monte Carlo (MC) simulation pipeline~\cite{Christensen:2008py,Alloul:2013bka,Degrande:2011ua,Alwall:2014hca}. This approach, a cornerstone of high-energy physics research, enables robust studies of BSM scenarios by emulating the entire data collection and processing chain of a collider experiment~\cite{Sjostrand:2014zea,deFavereau:2013fsa}. The key advantages of this methodology include~\cite{Alwall:2014hca,Cacciari:2011ma}:

\begin{itemize}
    \item The ability to perform automated calculations of theoretical quantities such as cross-sections and decay widths for complex processes.
    \item Rapid generation of large event samples with computational efficiency that enables extensive parameter space exploration and optimization studies.
    \item Conducting feasibility studies and optimizing analysis strategies prior to data acquisition.
    \item Estimating the efficiency of complex event selection criteria and the geometric acceptance of the detector.
    \item Predicting the rates and kinematical distributions of both irreducible and reducible background processes.
    \item Comparing and distinguishing between different theoretical hypotheses for a potential discovered signal.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Slides/2023_paper/Workflow.png}
    \caption{Schematic overview of the phenomenological MC pipeline. The workflow is divided into three stages: \textit{(green)} model definition and UFO export using FeynRules; \textit{(pink)} event generation from matrix elements through detector simulation using MadGraph, Pythia, Delphes, and PyRoot; \textit{(blue)} statistical analysis including event selection, signal-background discrimination, and significance calculation using Pandas, XGBoost, and ROOT statistical tools.}\label{fig:sim_workflow}
\end{figure}

The simulation workflow is modular, reflecting the logical progression from a theoretical Lagrangian to statistical significance estimates~\cite{Christensen:2008py,Alloul:2013bka,Degrande:2011ua,Alwall:2014hca}. As illustrated in Fig.~\ref{fig:sim_workflow}, it can be organized into three main stages: (i) theoretical model implementation (green region), (ii) event generation and detector simulation (pink region), and (iii) statistical analysis and signal extraction (blue region). To delve further into this pipeline, each stage is described in detail below~\cite{Alwall:2014hca,deFavereau:2013fsa}:

\begin{enumerate}
    \item \textbf{Model Implementation:} The process begins with the implementation of the theoretical model in \texttt{FeynRules} (v2.3.43)~\parencite{Christensen:2008py,Alloul:2013bka}. The Lagrangian of the new physics scenario, including all particle definitions, parameters, and interactions, is translated into a set of Feynman rules and exported in the Universal FeynRules Output (UFO) format~\cite{Degrande:2011ua}, interoperable with modern matrix-element generators~\cite{Alwall:2014hca}. 
    
    Alternative tools exist for model implementation and phenomenological studies, such as \texttt{SARAH}~\cite{Staub:2013tta} which can interface with spectrum generators like \texttt{SPheno}~\cite{Porod:2003um} to compute mass spectra, decay widths, flavor observables, and Wilson coefficients. These tools can also connect to specialized codes like \texttt{HiggsBounds/HiggsSignals}~\cite{Bechtle:2020pkv} for Higgs sector constraints, \texttt{FlavorKit}~\cite{Porod:2014xia} for flavor physics observables, \texttt{micrOMEGAs}~\cite{Belanger:2020gnr} for dark matter relic density and direct/indirect detection calculations, among many others. However, for the simplified models and collider-focused analyses considered in this work, the direct \texttt{FeynRules}-to-UFO workflow provides sufficient flexibility and computational efficiency.
    
    \item \textbf{Matrix Element Generation:} This UFO module, accompanied by a parameter card defining numerical values for masses and couplings, serves as input to \texttt{MadGraph5\_aMC@NLO} (v3.5.7)~\parencite{Alwall:2014bza,Alwall:2014hca}. Within \texttt{MadGraph}, the hard process and corresponding matrix elements are generated and stored in Les Houches Event (LHE) files. The PDF choices (here NNPDF3.0 NLO~\parencite{NNPDF:2014otw}) and matching/merging settings (MLM/CKKW-type) are configured to control radiation and multi-jet overlap~\cite{Alwall:2007fs,Buckley:2015}.
    
    To accurately model processes featuring significant interference effects between the new physics signal (e.g., a $\zb'$ boson) and the SM backgrounds, the full squared amplitude (often referred to as the Signal-Discriminated Events or SDE strategy) is employed for the phase-space integration. The \texttt{MadEvent} submodule generates unweighted parton-level events, stored in the LHE format, containing the four-momenta of all final-state particles. The generation is optimized through careful configuration of the \texttt{run\_card}, setting appropriate kinematic cuts on final-state partons to avoid wasting computational resources on events that would subsequently be rejected by the detector simulation.
    
    \item \textbf{Parton Shower and Hadronization:} Given the presence of additional jet radiation, the MLM matching scheme~\parencite{Alwall:2007fs} is applied to mitigate the double-counting of jet emission between the matrix element calculation and the subsequent parton shower. The parton-level LHE events are then passed to \texttt{PYTHIA} (v8.2.44)~\parencite{Sjostrand:2014zea} for the modeling of QCD and QED radiation (parton showering), hadronization, and particle decays. This step translates the colored partons into stable, color-singlet hadrons and resonances that form the observable final state. The resulting events, which include a full list of generator-level particles, are saved in the HepMC2 format.
    
    \item \textbf{Detector Simulation:} Detector effects are simulated using \texttt{DELPHES} (v3.4.2)~\parencite{deFavereau:2013fsa}, a fast parametric detector simulation framework. The \texttt{delphes\_card\_CMS.tcl} configuration card is used to emulate the response of the CMS detector, including the geometric acceptance, tracking efficiency, calorimeter energy resolution and segmentation, and the inner and outer magnetic field. Key reconstruction algorithms are applied within \texttt{DELPHES}:
    \begin{itemize}
        \item Jets are clustered from calorimeter towers using the anti-$k_t$ algorithm~\parencite{Cacciari:2008gp} with a distance parameter of $R=0.4$, and $b$-tagging is simulated based on the efficiency and mis-tag rate of the CMS performance.
        \item Muons and electrons are identified with efficiency maps that are functions of $p_T$ and $\eta$.
        \item The $\vec{p}_T^{\text{miss}}$ is calculated from the negative vector sum of all reconstructed particle momenta, as it was defined in Equation~\ref{eq:ptmiss}.
        \item Object calibrations (e.g., Jet Energy Corrections, lepton scale factors) are applied to align the simulation with the expected detector performance.
    \end{itemize}
    The final output, containing reconstructed physics objects (jets, leptons, $\abs{\vec{p}_T^{\text{miss}}}$), is stored in ROOT format~\parencite{Brun:1997pa}.
\end{enumerate}

At this stage, the analysis of the simulated samples converges with the methodology applied to real collider data. The subsequent steps, corresponding to the blue region of the workflow in Fig.~\ref{fig:sim_workflow}, involve the statistical analysis and signal extraction phase:

\begin{enumerate}
    \setcounter{enumi}{4}
    \item \textbf{Event Selection:} The ROOT ntuples containing reconstructed physics objects are converted into structured data formats (typically Pandas DataFrames~\parencite{mckinney2010data}) for efficient manipulation and analysis using PyROOT~\parencite{Brun:1997pa}. Physics-motivated selection criteria are applied to enhance the signal-to-background ratio, including kinematic cuts on transverse momenta, pseudorapidity ranges, invariant masses, and topological variables.

    \item \textbf{Signal-Background Discrimination:} Advanced multivariate analysis techniques, such as Boosted Decision Trees (BDT) or Gradient Boosting algorithms (e.g., XGBoost~\parencite{chen_xgboost_2016}), are employed to maximize the separation between signal and background events. These machine learning classifiers are trained on high-level kinematic features and exploit correlations between observables that are not captured by simple cut-based selections. The output discriminant provides an optimal observable for the final statistical test.

    \item \textbf{Significance Calculation:} Likelihood-based methods are used to extract signal yields, set exclusion limits, or estimate discovery significance, accounting for both statistical and systematic uncertainties through nuisance parameters. The results are typically expressed in terms of expected significance, confidence level exclusion limits (e.g., 95\% CL), or posterior probability distributions in Bayesian frameworks. The expected sensitivity to the signal hypothesis is usually quantified using statistical hypothesis testing frameworks implemented in ROOT (e.g., RooFit, RooStats)~\parencite{Butterworth:2015oua,Moneta:2010pm}. However, for this work, we preferred to use a custom implementation of the profile-binned likelihood test, as explained in Sec.~\ref{sec:power_analysis}.
\end{enumerate}

The reliability of the simulation is validated by comparing the modeling of well-known SM processes (e.g., Drell-Yan, $t\bar{t}$ production) against published results and data-driven control regions. Dominant theoretical systematic uncertainties, such as those arising from the choice of factorization and renormalization scales, PDF variations, and parton shower modeling, are evaluated and propagated through the analysis.


