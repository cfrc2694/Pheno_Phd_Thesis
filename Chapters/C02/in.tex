\chapter[Phenomenological Framework]{Phenomenological Framework for LHC Searches and Analysis}

Since its formulation, the Standard Model (SM) has proven remarkably successful in describing the fundamental particles and interactions, and its parameters have been measured with increasing precision over several decades. However, as discussed in the previous chapter, various theoretical and experimental observations suggest that the SM is incomplete. As outlined previously, this is motivated by theoretical shortcomings—such as the hierarchy problem, the absence of a dark matter candidate, and non-zero neutrino masses—as well as by experimental anomalies. These limitations motivate the search for new physics (NP) beyond the SM (BSM).

The search for BSM physics proceeds along two main axes: the construction of theoretical extensions to the SM, and the development of experimental methods to probe them. A necessary condition for any viable BSM model is consistency with existing experimental data, which places strong constraints on its parameter space. These constraints include lower limits on the masses of new particles from direct searches at high-energy colliders, and upper bounds on couplings and mixing angles from precision measurements at both high and low energies, which are sensitive to virtual corrections.

Phenomenology connects theoretical models to experimental observables by calculating cross sections, decay rates, and other signatures for given model parameters. A critical function of this field is to assess the experimental feasibility of BSM scenarios—evaluating whether predicted signals would be observable above background processes given the capabilities of current and future detectors. This involves estimating production rates, modeling detector acceptance and efficiency, and developing discrimination variables to maximize signal-to-background ratios. This feasibility assessment is essential for designing analysis strategies, particularly at the Large Hadron Collider (LHC), where signals of new physics must be discriminated from large Standard Model backgrounds.

This feasibility assessment is essential for designing analysis strategies at the Large Hadron Collider (LHC), a proton-proton ($pp$) collider operating since 2009. The LHC has provided data at center-of-mass energies from $7~\mathrm{TeV}$ to $13.6~\mathrm{TeV}$. During Run~I (2010–2013), operations at $7$–$8~\mathrm{TeV}$ led to the discovery of the Higgs boson using a dataset corresponding to an integrated luminosity of roughly $30~\mathrm{fb}^{-1}$. Run~II (2015–2018) significantly expanded this dataset, collecting approximately $140~\mathrm{fb}^{-1}$ at $13~\mathrm{TeV}$. Run~III (2022–2025) is currently underway at $13.6~\mathrm{TeV}$ and is more than doubling the available data, with a target of over $300~\mathrm{fb}^{-1}$. Future operations will be dominated by the High-Luminosity LHC (HL-LHC), starting around 2029, which is designed to accumulate an unprecedented integrated luminosity of $3000~\mathrm{fb}^{-1}$. This vast increase in data volume enables searches for exceedingly rare processes but also requires discriminating potential signals of new physics from correspondingly large and complex SM backgrounds, making sophisticated phenomenological tools increasingly important.

\input{Chapters/C02/s01.tex} 
\input{Chapters/C02/s02.tex} 
\input{Chapters/C02/s03.tex} 
\input{Chapters/C02/s04.tex} 
